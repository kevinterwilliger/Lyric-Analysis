---
title: "Predicting Song Popularity from Characteristics of Lyrics"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
setwd("~/Desktop/GitHub/Econ490/Report3")
# setwd("C:/Users/Kevin/Desktop/Github/Econ490/Report3")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(stringr)
library(SentimentAnalysis)
library(ggplot2)
library(dotwhisker)
library(stats)
library(xtable)
#library(reticulate)
new_data = read.csv("Data_and_feature.csv",header=T)
```

# Table of Contents (Clickable!)

1. **[Report 1.](#Report1)**

  + *[Introduction](#intro1)*
  
  + *[Cleaning the Data](#clean)*
  
  + *[Examining the Data](#examine)*
  
  + *[Correlation of Variables](#corr)*
  
  + *[Conclusion](#conc)*

2. **[Report 2.](#Report2)**

  + *[Introduction](#intro2)*
  
  + *[Regressions](#reg)*
    
    - [Rank](#rank)
    
      * [Average Word Length](#awl)
    
      * [Average Rhyme Length](#arl)
    
      * [Number of Words](#nw)
    
      * [Unique Words](#uw)
      
      * [Sentiment](#sent)
      
    - [Genre](#genre)
    
      * [Cleaning Genre Data](#clean2)
      
      * [Predicting Pop Songs](#pop)
      
      * [Predicting Rock Songs](#rock)
      
      * [Predicting Audio Analysis](#aa)
      
      * [Standard Errors](#se)
      
      * [Probit Regression](#pr)
      
  + *[Conclusion](#conc2)*
    
\newpage

## Report 1 {#Report1}

### Introduction {#intro1}
I listen to a lot of music. I've never really had a musical bone in my body so I tend to gravitate toward the lyrics of a song (which is a reason why you'll never find an EDM song in my playlists). After finding a dataset on Kaggle containing the lyrics of Billboard's Hot100 year-end chart from 1964-2015, I decided that it would be interesting to use the lyrics to try to predict a given song's success on the BillBoard Hot100. The Hot100 is just a list of the most popular songs of the given year ranked from 1 (most popular) to 100 (100-most popular).

### Cleaning the Data {#clean}

Here's what my original data set looked like.
```{r}
original_data = read.csv("../Report1/billboard_lyrics_1964-2015.csv")
names(original_data)
# head(original_data)
print(xtable(original_data[1:5,]),type = "latex",comment = F)
```

I decided that I wanted to come up with more columns to better interpret lyrics and had to come up with some independent variables on my own. I wrote a python script to help me. The script, which is below, imports the lyrics from the data set and finds the number of words, average word length, average rhyme length, and number of unique words in the given lyrics. 

(I've moved the code to [this github](https://github.com/kevinterwilliger/Lyric-Analysis) if you want to see the new and updated python scripts.)  

*In case links don't work in pdf: https://github.com/kevinterwilliger/Lyric-Analysis*  
  

After saving as a csv, I imported the new data frame into R and used a package called [SentimentAnalysis](https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html#sentimentdictionarywordlist) to calculate a sentiment score for each of the lyrics. Here's the code:

```{r}
data1 = read.csv("data_new.csv")
data1 = data1 %>% 
  mutate("Sentiment" = analyzeSentiment(as.character(Lyrics))$SentimentQDAP) %>%
  rename("AverageWordLength" = AverageLengths)

# write.csv(data,"data_clean.csv")
```

Here are the Xs broken down:

1. **Number of words** - This one should be pretty self-explanatory: How many words (non-unique included) does the song contain?

2. **Average Word Length** - Does the songwriter use lengthier words or smaller words? The script gets this by adding all the characters in the lyrics and dividing by the total number of words in the lyrics.

3. **Average Rhyme Length** - I had to use a handy library I found to compute this one. I'll link the github at the end, but here's a quick rundown of how it works: &nbsp;
  * Go through lyrics word for word, using eSpeak to get the phonetic make-up of each word.

  * For every word, find the longest matching rhyme sequence from the last 15 words. This is what captures the length of the rhyme. Since eSpeak converts all the words to phonetics (and I think just vowel phonetics) only vowels are compared, which is what gives the program something to match. Essentially, the program searches for the longest matching vowel sequence for last X words, where I used an arbitrary 15 words for X.

  * Find the average rhyme length by summing all the lengths and dividing by the number of rhymes.
[Here's the github](https://github.com/ekQ/raplysaattori)

4. **Number of Unique Words** - I used python's dictionaries to keep track of the number of unique words in each song. Easy enough.

5. **Sentiment** - I'm not entirely sure what algorithm is used in this package, but I used the analyzeSentiment function from  [SentimentAnalysis](https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html#sentimentdictionarywordlist) to get each number. The sentiment is a score that gives the general "emotion" score of a word or series of words from -1 to 1. A negative score correlates to a more negative emotion, whether angry, sad, or both. Positive scores would work in the reverse, the higher the score, the more positive the sentiment expressed in the lyrics is.
 
### Examining the Data {#examine}

I'll plot the three most interesting of the X's that I mentioned above: Average Word Length, Average Rhyme Length, and Sentiment. Let's start with Average Word Length.

```{r}
data2 = read.csv("data_clean.csv")
head(data2)
plot = ggplot(data2,aes(x=AverageWordLength)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=30) +
  geom_density(alpha=.2, fill="#FF6666") +
  labs(title = "Histogram of Average Word Length per Song",x="Average Word Length",y="Density") +
  theme_minimal()
plot
```

From the plot, we can see that the average word length in each song tends to be between 3 letters and 5 letters. I wonder how this histogram differs from the average word length of all english words. My best guess is that this graph is shifted left: not that many songs use words like supercalifragilisticexpialidocious. Next, Average Rhyme Length.

```{r}
plot = ggplot(data2,aes(x=AverageRhymeLength)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=30) +
  geom_density(alpha=.2, fill="#FF6666") +
  labs(title = "Histogram of Average Rhyme Length per Song",x="Average Rhyme Length",y="Density") + theme_minimal()

plot
```

So the average rhyme length in the songs is around 1 vowel-phonetic per rhyme. I am curious to see how this might correlate to genre (as a hip-hop fan, I hope that rap might have longer rhymes than pop or country). 

And Sentiment.

```{r}
plot = ggplot(data2,aes(x=Sentiment)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=30) +
  geom_density(alpha=.2, fill="#FF6666") +
  labs(title = "Histogram of Sentiment of Song",x="Sentiment",y="Density") +
  theme_minimal()
plot
```

So most songs tend to skew positive, I guess that's not terribly surprising. And finally, Rank on Billboard Hot100. This is technically a discrete variable, but it has 100 values so a histogram works better. 

```{r}
plot = ggplot(data2,aes(x=Rank)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=100) +
  geom_density(alpha=.2, fill="#FF6666") +
  labs(title = "Histogram of Rank of Songs on Billboard Hot100",x="Rank",y="Density") +
  theme_minimal()
plot
# I am curious why the histogram isn't totally uniform, as it should be. TODO: Find out why
```

The question above actually has two answers. The first is because this "data" is cleaned to remove any junk values I appended in the script, therefore having less that 50*100 rows and a non-uniform histogram. The other answer I will show below.

```{r}
# get the count of every song/artist combination
proof = data2 %>%
  mutate(track = paste(Song,Artist,sep=" ")) %>%
  group_by(track) %>%
  summarise(n = n()) %>%
  filter(n > 1)
# Checking for duplicates
proof
```

So there are 195 duplicates. Out of curiosity, are these duplicates an error by whoever created the dataset?

```{r}
data2$Year[data2$Song == "100 pure love"]
```

I'm going to say that the creator did nothing wrong and that the vast majority, if not all, duplicates are caused by songs being popular for more than just one year. I think I will remove duplicates in the future.

```{r echo=F,include=T}
data_ = read.csv("data_and_feature.csv",header=T)
new = data_ %>%
  mutate(track = paste(Song,Artist,sep=" "))

data2 = new[!new$track %in% proof$track,] %>% select(-c(X.2,X.1,X,Unnamed..0))
# write.csv(data2,"data_and_feature_Clean.csv")
```


### Correlation of Variables {#corr}

```{r,warning=F}
d = data2 %>% dplyr::select(Rank,AverageWordLength,AverageRhymeLength,NumberWords,UniqueWords,Sentiment)
scatter.smooth(d$Rank ~ d$AverageWordLength,data=d)
```

Not a high correlation.

```{r}
scatter.smooth(d$Rank ~ d$AverageRhymeLength)
```

Agian, low.

```{r}
scatter.smooth(d$Rank ~ d$Sentiment)
```

Yeah, so not good. I can't say I'm surprised, but there has got to be a way for me to get better predictions from these variables. First, maybe a binary prediction variable will prove better.

```{r}
binary = data_ %>%
  mutate(Top50 = ifelse(Rank <= 50,1,0)) %>%
  select(Top50,AverageWordLength,AverageRhymeLength,Sentiment,NumberWords,UniqueWords)

plot = ggplot(binary,aes(x=Top50)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white",bins=100) +
  geom_density(alpha=.2, fill="#FF6666") +
  labs(title = "Histogram of Binary Variable Top50",x="Top50",y="Density") +
  theme_minimal()
plot
```

OK good, a 50-50(ish) split is what we wanted.

```{r echo=F,error=F,warning=F}
boxplot(AverageWordLength ~ Top50,data=binary)
```

Oof. Not good.

```{r warning=F}
boxplot(AverageRhymeLength ~ Top50,data=binary)
```

This is not an improvement.

```{r warning=F}
boxplot(Sentiment ~ Top50,data=binary)
```


Yeah so not better. In the next report, I will be adding a few more columns, one of them being genre of the song. I will probably try to predict genre from the other columns, hopefully it will work better.

### Concluding Thoughts {#conc}

I really want to make this project work. So far it seems likely that the linear regressions and logistic regressions in report 2 will prove to be inept at predicting Rank or Top50. As I write this conclusion, I am trying to finalize the script I mentioned above, but it relys on some APIs that are proving to be difficult. I think that trying to predict genre from variables like average rhyme length or unique words might prove more successful than predicting popularity. If the graders have any recommendations on possible Y/X variables, I'd very much appreciate the outside input. Other than that, here is report 2!


## Report 2 {#Report2}
```{r include=F}
data = read.csv("data_and_feature_Clean.csv",header = T)
```

### Introduction {#intro2}

Due to the fact that any linear regressions trying to predict the popularity of a song from the aforementioned independent variables are very underwhelming, I wrote some more python scripts (that can be found at [https://github.com/kevinterwilliger/Lyric-Analysis](https://github.com/kevinterwilliger/Lyric-Analysis)) to collect more variables such as the key or mode of a song and the genres of a song. I used the [Spotipy](https://spotipy.readthedocs.io/en/latest/) library to collect an audio analysis of the song that contains the following variables:

1. **Loudness** - The overall loudness (decibels, dB) of a song.

2. **Tempo** - The beats per minute of the song.

3. **Key** - The estimated overall key of the song. Ranges in values from 0-11 mapping to pitches, using the standard Pitch Class Notation.

4. **Mode** - Binary variable where "0" is Minor and "1" is in Major

5. **Time Signiture** - The "meter" of a song is measured by the number beats in each bar. Values range from 3-7 and indicate "$\frac{3}{4}$"-"$\frac{7}{4}$".

Using the Last.FM API, I was able to collect the top 5 user tags corresponding to each song. I collected 5 tags due to the fact that tags do not always contain the genre, but most songs have the specific genre in one of the 5 tags. I will explain my process for narrowing the tags down further in the report.



### Regressions {#reg}

#### Rank {#rank}

Let's start with running some regressions on Rank. The results are not great, but I expected this due to the fact popularity probably is not dictated by the contents of the lyrics. We'll touch on Genre after.

##### Average Word Length {#awl}

```{r}
fit = lm(Rank ~ AverageWordLength, data=data)
summary(fit)
```

From the above summary, the R-squared of the linear regression of average word length on rank is 0.00005306. This is such a bad R-squared that it is clear that average word length has almost no bearing on how popular a song will be.

##### Average Rhyme Length {#arl}

```{r}
fit = lm(Rank ~ AverageRhymeLength,data=data)
summary(fit)
```

Again, a terrible R-squared. It is interesting, though, that average rhyme length has a tiny negative relationship toward rank.

##### Number of Words {#nw}

```{r}
fit = lm(Rank ~ NumberWords, data=data)
summary(fit)
```

I would expect a small or negative relationship from this due to the fact that long songs do not often recieve as many listens as shorter songs.

##### Number of Unique Words {#uw}

```{r}
fit = lm(Rank ~ UniqueWords, data = data)
summary(fit)
```

This is the most significant variable, it being able to pass a 85% significance test. It is a shame, though, that the R-squared is basically zero.

##### Sentiment {#sent}

```{r}
fit = lm(Rank~Sentiment,data=data)
summary(fit)
```

So none of my lyrics columns are good predictors of Rank.

#### Genre {#genre}

##### Cleaning Genre Data {#clean2}

Next, let's use genre as the variable we are trying to predict.

First we have to clean the genre data and create a singular genre column.

```{r definition,include=F}
rap <- function(genre) {
  l = c("rap","hiphop","hip hop","hip-hop")
  return(genre %in% l)
}
```


```{r}
tags = read.csv("genres.csv",header=T)

# This has got to be the least efficient way to do this so if you have any suggestions, I am very open.
list = c("rock","folk","country","/^hip*[a-z]./","soul","r&b","classic rock","smooth jazz",
         "jazz","garage rock","punk","metal","folk rock","british","surf","fip","rap","hip hop",
         "rnb","indie","indie rock","pop rock","soft rock","psychadelic",
         "psychadelic","rock","electronic","synth pop","dance","funk",
         "reggae","progressive rock","punk rock","hip-hop","love","hiphop","house","pop")


# Find tags that match to those in list and put in genre column
genres = tags %>% 
  mutate(genre = ifelse(str_trim(tolower(Tag1)) %in% list, str_trim(tolower(Tag1)),
                        ifelse(str_trim(tolower(Tag2)) %in% list, str_trim(tolower(Tag2)),
                               ifelse(str_trim(tolower(Tag3)) %in% list, str_trim(tolower(Tag3)),
                                      ifelse(str_trim(tolower(Tag4)) %in% list, str_trim(tolower(Tag4)),
                                             ifelse(str_trim(tolower(Tag5)) %in% list, str_trim(tolower(Tag5)),9999)))))) %>% 
  select(genre)


old = read.csv("data_and_feature.csv",header=T)
data_genres = cbind(old,genres) %>% mutate(track = paste(Song,Artist,sep=" "))
  
clean_full = data_genres[data_genres$genre != 9999,] %>% 
  mutate(genre = ifelse(rap(genre),"rap",genre),
         track = paste(Song,Artist,sep = " ")) %>%
  filter(track %in% data$track)

```

Due to the nature of the API, I ended up dropping around 187 songs, but that leaves us with 3931 rows of song data, genre included. Lets look at the spread of the genres.

```{r}
num_genre = clean_full %>% group_by(genre) %>% summarise(count=n()) %>% select(genre,count)

genreplot = num_genre[num_genre$count > 20,] %>%
  ggplot(., aes(x=genre, y=count, fill=genre)) +
  geom_bar(stat="identity")+theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = 'none')

genreplot
```

There are about 1200 pop songs, but pop is such a wide ranging genre that this should be somewhat expected. Other than pop, it seems that these 17 genres are pretty all-representing.

This cleans the genres with under 20 songs and creates binary columns representing the 17 genres.

```{r}

genre_list = num_genre[num_genre$count > 20,] %>% mutate(X = row_number())

clean_cut = clean_full[clean_full$genre %in% genre_list$genre,] %>% 
  mutate(X = row_number())


binary = as.data.frame(clean_cut %>% select(X,genre) %>% model.matrix(~.,data=.))

final_data = right_join(clean_cut,binary,by="X",keep=F)
colnames(final_data)
head(final_data$Loudness)
```


Now that we have our genre columns, we can try to predict the genre from the different independent variables.

##### Predicting Pop songs {#pop}

First lets try just the lyric data.

```{r}
pop = final_data %>% select(genrepop,Rank,AverageWordLength,AverageRhymeLength,NumberWords,UniqueWords,Sentiment)

fit = glm(genrepop ~ Rank + AverageWordLength + AverageRhymeLength + NumberWords + UniqueWords + Sentiment,data=pop,family = "binomial")
summary(fit)
```

From this it seems like Number of Words, Unique Words, and Rank are all significant. Let's drop the insignificant variables.

```{r}
pop = pop %>% select(Rank,genrepop,NumberWords,UniqueWords)

fit = glm(genrepop ~ Rank + NumberWords + UniqueWords,data=pop,family=binomial)
summary(fit)
```

And let's then create a confusion matrix.

```{r}
probs = predict(fit,type="response")
prediction = predict(fit)
prediction = rep("Not Pop",nrow(pop))
prediction[probs > .5]="Pop"
real = recode(pop$genrepop,"1"="Pop","0"="Not Pop")
table(prediction,real)
```

The confusion matrix tells us that the model predicts that a song is not pop very accuratly, but seems unable to predict when a song is pop. Let's see if this is the case for other genres too.

##### Predicting Rock Songs {#rock}

Using the same logic from above, here is the fit summary for predicting whether a song is rock or not.

```{r}
rock = final_data %>% select(genrerock,Rank,AverageWordLength,AverageRhymeLength,NumberWords,UniqueWords,Sentiment)

fit = glm(genrerock ~ Rank + AverageWordLength + AverageRhymeLength + NumberWords + UniqueWords + Sentiment,data=rock,family = "binomial")
summary(fit)
```

Interestingly, there are different significant variables. Rank and Number of Words are the same, but this time sentiment is significant. What is interesting is that the coefficient for sentiment is much larger than any other variable from rock or pop.

```{r}
rock = rock %>% select(Rank,genrerock,NumberWords,Sentiment)

fit = glm(genrerock ~ Rank + NumberWords + Sentiment,data=rock,family=binomial)
summary(fit)
```

And let's then create a confusion matrix.

```{r}
probs = predict(fit,type="response")
prediction = predict(fit)
prediction = rep("Not Rock",nrow(rock))
prediction[probs > .5]="Rock"
real = recode(rock$genrerock,"1"="Rock","0"="Not Rock")
table(prediction,real)
unique(prediction)
```

The last function I call is to prove that the model only predicts "Not Rock" (0) for any input in the data set. Due to that, the model can't be a good predictor if it can't predict that a rock song is a rock song. I'm going to remove pop songs from the data set to see if it helps.

```{r}
notPop = final_data[final_data$genrepop == 0,]

rock = notPop %>% select(genrerock,Rank,AverageWordLength,AverageRhymeLength,NumberWords,UniqueWords,Sentiment)

fit = glm(genrerock ~ Rank + NumberWords + Sentiment, data=rock,family = binomial)
summary(fit)

probs = predict(fit,type="response")
prediction = predict(fit)
prediction = rep("Not Rock",nrow(rock))
prediction[probs > .5]="Rock"
real = recode(rock$genrerock,"1"="Rock","0"="Not Rock")
table(prediction,real)
unique(prediction)
```

This time, the model predicted "Rock" (1) one time, which was a false positive. I'll consider this model bust.

##### Predicting Audio Analysis {#aa}

Now I am going to incorporate results from my Spotify API into these logisitc regressions. Hip-Hop is my favorite type of music, and tends to have pretty unique musical traits, so I'll use rap as a prediction variable out of non-pop songs.

```{r}
aa = final_data[(final_data$Loudness != 9999) && (final_data$Tempo != 9999),]

fit = glm(genrepop ~ UniqueWords + Loudness:Tempo,data=aa,family = binomial)
summary(fit)
```

Both variables are signficantly different from 0 but their coefficients are so small that any change in the variable has negligible effect. It is interesting that unique words has a negetive coefficient, though. Maybe because Pop songs often repeat the same phrase or word often. 

And the confusion table.


```{r}
probs = predict(fit,type="response")
prediction = predict(fit)
prediction = rep("Not Pop",nrow(aa[(aa$Loudness != 9999)&&(aa$Tempo != 9999),]))
prediction[probs > .5]="Pop"
real = recode(aa$genrepop,"1"="Pop","0"="Not Pop")
table(prediction,real)
```

```{r}
# Percent Correct
mean(prediction==real)

# False Positive Rate
20/23

# True Positive Rate
2540/2560
```


For the sake of shortness, I ran a bunch of different combinations and this is the best I could predict. The model rarely predicts that a song is "Pop", but has a high false positive rate. Because the model mostly outputs "Not Pop" it has a high true positive rate - 66% of the data was "Not Pop".


##### Standard Errors {#se}

As I mentioned above, the coefficients are very small, though significantly different from zero.

```{r}
dwplot(fit,vline = geom_vline(xintercept = 0, colour = "grey60", linetype = 2))
```

##### Probit Regression {#pr}

Here is a probit regression for comparison.

```{r}
fit = glm(genrepop ~ UniqueWords + Loudness:Tempo,data=aa[(aa$Loudness != 9999)&&(aa$Tempo != 9999),],family = binomial(link="probit"))
#summary(fit)
probs = predict(fit,type="response")
prediction = predict(fit)
prediction = rep("Not Pop",nrow(aa[(aa$Loudness != 9999)&&(aa$Tempo != 9999),]))
prediction[probs > .5]="Pop"
real = recode(aa$genrepop,"1"="Pop","0"="Not Pop")
table(prediction,real)
```

It seems that the coefficients in the probit regression are different from the logit, but still so small that they are mostly negligible.

### Conclusions {#conc2}

I had hoped that the results with the new independent columns would be better predictors of genre, but it seems that my data is not well enough suited to predict anything from the data. 

I'm not really sure where I'll go from here, but I think I can find ways to better predict genre or rank.
